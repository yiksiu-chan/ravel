{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'content/models'\n",
    "DATA_DIR = 'content/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fc0fd3792a4e1c846379eff20b71f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e2ce9f1297492ab31e4fe40782e389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "load_tokenizer_only = False\n",
    "\n",
    "cache_dir = MODEL_DIR\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "\n",
    "if not load_tokenizer_only:\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, low_cpu_mem_usage=True, device_map='auto', cache_dir=cache_dir,\n",
    "      torch_dtype=torch.bfloat16)\n",
    "  model = model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and edit datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the city entites from Huggingface\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ravel_city_entities = load_dataset(\"hij/ravel\", \"city_entity\")\n",
    "ravel_city_prompts = load_dataset(\"hij/ravel\", \"city_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MIB, use only the country, continent, and language\n",
    "\n",
    "# Remove other attributes from ravel_city_entities\n",
    "from datasets import DatasetDict\n",
    "columns_to_remove = [\"Latitude\", \"Longitude\", \"Timezone\", \"URL\"]\n",
    "ravel_city_entities = ravel_city_entities.map(lambda x: x, remove_columns=columns_to_remove)\n",
    "\n",
    "# Remove other attributes from ravel_city_prompts\n",
    "attributes_to_remove = {\"Latitude\", \"Longitude\", \"Timezone\"}\n",
    "ravel_city_prompts = ravel_city_prompts.filter(lambda x: x[\"Attribute\"] not in attributes_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Group prompts by attribute.\n",
    "attribute_prompts = collections.defaultdict(list)\n",
    "prompt_splits = {}\n",
    "for split in ravel_city_prompts:\n",
    "  for prompt in ravel_city_prompts[split]:\n",
    "    attribute_prompts[prompt['Attribute']].append(prompt['Template'])\n",
    "    prompt_splits[prompt['Template']] = split\n",
    "attribute_prompts = dict(attribute_prompts)\n",
    "\n",
    "# Build entity to attributes mapping.\n",
    "attributes = ['Country', 'Continent', 'Language'] #, 'Latitude', 'Longitude', 'Timezone']\n",
    "entity_splits = {}\n",
    "entity_attributes = collections.defaultdict(list)\n",
    "for split in ravel_city_entities:\n",
    "  for entity in ravel_city_entities[split]:\n",
    "    entity_attributes[entity['City']].append({k: v for k, v in entity.items() if k in attributes})\n",
    "    entity_splits[entity['City']] = split\n",
    "entity_attributes = dict(entity_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "entity_type = 'city'\n",
    "\n",
    "print(f'#entities={len(entity_attributes)}, #prompt_templates={sum(map(len, attribute_prompts.values()))}')\n",
    "\n",
    "prompts_to_meta_data = {t % x: {'entity': x, 'attr': a, 'template': t}\n",
    "               for x in entity_attributes\n",
    "               for a, ts in attribute_prompts.items()\n",
    "               for t in ts\n",
    "               # An empty attribute means the prompt is from Wikipedia, which\n",
    "               # does not query for a specific attribute.\n",
    "               if a != ''}\n",
    "print(len(prompts_to_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.generation_utils import generate_batched\n",
    "\n",
    "# prompt_max_length = 48\n",
    "\n",
    "# prompt_to_output = generate_batched(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     list(prompts_to_meta_data),\n",
    "#     prompt_max_length+8,\n",
    "#     prompt_max_length=prompt_max_length,\n",
    "#     batch_size=64)\n",
    "# prompt_to_output = {k: v[len(k):] for k, v in prompt_to_output}\n",
    "\n",
    "# torch.save(prompt_to_output, os.path.join(DATA_DIR, 'ravel_gemma-2-2b_city_prompt_to_output.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3615235/580774497.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prompt_to_output = torch.load(os.path.join(DATA_DIR, 'ravel_gemma-2-2b_city_prompt_to_output.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "415226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly load the pre-computed outputs.\n",
    "prompt_to_output = torch.load(os.path.join(DATA_DIR, 'ravel_gemma-2-2b_city_prompt_to_output.pt'))\n",
    "len(prompt_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average accuracy: 99.10%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Behavioral Test\n",
    "\n",
    "# import collections\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# from zoneinfo import ZoneInfo\n",
    "# import datetime\n",
    "\n",
    "# def timezone_name_to_utc_offset(name):\n",
    "#   try:\n",
    "#     offset =  ZoneInfo(name).utcoffset(datetime.datetime.now()).seconds\n",
    "#   except:\n",
    "#     return 'NOT_FOUND'\n",
    "#   sign = '+'\n",
    "#   if offset // 3600 >= 12:\n",
    "#     offset = 24 * 3600 - offset\n",
    "#     sign = '-'\n",
    "#   fmt_offset = str(datetime.timedelta(seconds=offset)).rsplit(':', 1)[0]\n",
    "#   if fmt_offset.startswith('0') and offset >= 1800:\n",
    "#     fmt_offset = fmt_offset[1:]\n",
    "#   return f'{sign}{fmt_offset}'\n",
    "\n",
    "\n",
    "# sorted_entity = sorted(set([v['entity'] for v in prompts_to_meta_data.values()]))\n",
    "# sorted_template = sorted(set([v['template'] for v in prompts_to_meta_data.values()]))\n",
    "# stats = np.zeros([len(sorted_entity), len(sorted_template)])\n",
    "# for p, out in prompt_to_output.items():\n",
    "#   attr = prompts_to_meta_data[p]['attr']\n",
    "#   entity = prompts_to_meta_data[p]['entity']\n",
    "#   # Each entity might be mapped to multiple attribute values.\n",
    "#   label = '|'.join(set([x[attr] for x in entity_attributes[entity] if x[attr]]))\n",
    "#   if not label:\n",
    "#     continue\n",
    "#   norm_label = label.lower()\n",
    "#   norm_out = out.split('\"')[0].strip(' \"').replace('\\\\/', '/').lower()\n",
    "#   if len(norm_label) < len(norm_out):\n",
    "#     correct = int(norm_out.startswith(norm_label))\n",
    "#   else:\n",
    "#     correct = int(norm_label.startswith(norm_out))\n",
    "\n",
    "  # Exceptions\n",
    "#   if re.search('coord|\"lat\"|\"long\"|latitude|coordinates|longitude', p):\n",
    "#     try:\n",
    "#       correct = int((float(norm_label.strip('-−')) - float(re.findall(r'\\d+', norm_out)[0])) <= 2)\n",
    "#     except:\n",
    "#       correct = 0\n",
    "  # if re.search('United States|United Kingdom', label):\n",
    "  #   norm_label = label.strip().replace('the ', '')\n",
    "  #   norm_out = out[len(p):].strip().replace('the ', '')\n",
    "  #   correct = int(norm_out.startswith(norm_label) or norm_out.startswith('England'))\n",
    "  # if re.search('South Korea', label):\n",
    "  #   correct = int(norm_out.startswith('korea') or norm_out.startswith('south korea'))\n",
    "  # if re.search('North America', label):\n",
    "  #   correct = norm_label in norm_out or norm_out == 'na' or norm_out.startswith('america')\n",
    "  # if re.search('Mandarin', label):\n",
    "  #   correct = norm_out in norm_label or norm_out == 'chinese'\n",
    "  # if re.search('language', p) and ',' in norm_label:\n",
    "  #   correct = any(lang in norm_out for lang in norm_label.split(','))\n",
    "#   if re.search('UTC', p) and '/' in norm_label:\n",
    "#     norm_label = [timezone_name_to_utc_offset(l) for l in label.split('|')]\n",
    "#     correct = any(norm_out.startswith(l.split(':')[0]) for l in label.split('|'))\n",
    "#     if not correct and re.search(r'[+\\-]0\\d', norm_out):\n",
    "#       correct = any(norm_out.replace('0', '', 1).startswith(l.split(':')[0]) for l in norm_label)\n",
    "#     norm_label = '|'.join(norm_label)\n",
    "#     # Summer daylight saving time\n",
    "#     if not correct and (\n",
    "#         re.search(r'\\-[5-8]', norm_label) and label.startswith('America') or\n",
    "#         re.search(r'\\+[0-3]', norm_label) and label.startswith('Europe') or\n",
    "#         re.search(r'\\+[0-3]', norm_label) and label.startswith('Africa')):\n",
    "#       #print('SUMMER TIME:', norm_label, norm_out)\n",
    "#       out_offset_match = re.search(r'[+\\-]?(\\d\\d?):\\d+', norm_out)\n",
    "#       label_offset_match = re.search(r'[+\\-]?(\\d\\d?):\\d+', norm_label)\n",
    "#       if out_offset_match and label_offset_match:\n",
    "#         norm_out_offset = int(out_offset_match.group(1))\n",
    "#         norm_label_offset = int(label_offset_match.group(1))\n",
    "#         correct = (norm_out_offset <= norm_label_offset + 1 and\n",
    "#                    norm_out_offset >= norm_label_offset - 1)\n",
    "#     if not correct and re.search(r'[+\\-](\\d+)', norm_out) and int(\n",
    "#         re.search(r'[+\\-](\\d+)', norm_out).group(1)) > 11:\n",
    "#       offset = 24 - int(re.search(r'[+\\-](\\d+)', norm_out).group(1))\n",
    "#       correct = str(offset) in norm_label\n",
    "#   stats[sorted_entity.index(prompts_to_meta_data[p]['entity']), sorted_template.index(prompts_to_meta_data[p]['template'])] += int(correct)\n",
    "\n",
    "# print('-----------------------------------')\n",
    "# for i in np.argsort(stats.sum(axis=0))[::-1]:\n",
    "#   print(sorted_template[i], int(stats[:, i].sum()), len(stats[:, i]))\n",
    "# for i in np.argsort(stats.sum(axis=-1))[::-1]:\n",
    "#   print(sorted_entity[i], int(stats[i].sum()), len(stats[i]))\n",
    "\n",
    "# # Keep the top K entities and templates.\n",
    "# num_entity = 800\n",
    "# num_template = 200\n",
    "# kept_entity_index = np.argsort(stats.sum(axis=1))[-num_entity:]\n",
    "# KEPT_ENTITY = [sorted_entity[i] for i in kept_entity_index]\n",
    "# topk_template_index = set(np.argsort(stats.sum(axis=0))[-num_template:])\n",
    "# kept_template_index = []\n",
    "# # A dict of all kept attribute to prompts.\n",
    "# KEPT_ATTR_TO_PROMPT_AND_SPLIT = {}\n",
    "# for attr in attribute_prompts:\n",
    "#   if not attr:\n",
    "#     # Wikipedia prompts.\n",
    "#     continue\n",
    "#   # Kept the top 4 to 12 templates per attribute.\n",
    "#   attr_indices = [sorted_template.index(t) for t in attribute_prompts[attr]]\n",
    "#   per_attr_kept_template_index = sorted(attr_indices, key=lambda i: stats[:, i].sum())[-12:][::-1]\n",
    "#   per_attr_kept_template_index = [x for i, x in enumerate(per_attr_kept_template_index)\n",
    "#                                   if x in topk_template_index or i < 4]\n",
    "#   kept_template_index.extend(per_attr_kept_template_index)\n",
    "#   KEPT_ATTR_TO_PROMPT_AND_SPLIT[attr] = {sorted_template[i]: prompt_splits[sorted_template[i]]\n",
    "#                                for i in per_attr_kept_template_index}\n",
    "# print('Kept %d entity, %d prompt template' % (len(kept_entity_index), len(kept_template_index)))\n",
    "\n",
    "# display('Average accuracy: %.2f%%' % (100 *  (stats[:, kept_template_index][kept_entity_index, :]).sum()/ (len(kept_entity_index) * len(kept_template_index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply checking model accuracy without filtering incorrect instances\n",
    "# (For MIB, no filtering in the data generation stage)\n",
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "import datetime\n",
    "\n",
    "def timezone_name_to_utc_offset(name):\n",
    "  try:\n",
    "    offset =  ZoneInfo(name).utcoffset(datetime.datetime.now()).seconds\n",
    "  except:\n",
    "    return 'NOT_FOUND'\n",
    "  sign = '+'\n",
    "  if offset // 3600 >= 12:\n",
    "    offset = 24 * 3600 - offset\n",
    "    sign = '-'\n",
    "  fmt_offset = str(datetime.timedelta(seconds=offset)).rsplit(':', 1)[0]\n",
    "  if fmt_offset.startswith('0') and offset >= 1800:\n",
    "    fmt_offset = fmt_offset[1:]\n",
    "  return f'{sign}{fmt_offset}'\n",
    "\n",
    "sorted_entity = sorted(set([v['entity'] for v in prompts_to_meta_data.values()]))\n",
    "sorted_template = sorted(set([v['template'] for v in prompts_to_meta_data.values()]))\n",
    "stats = np.zeros([len(sorted_entity), len(sorted_template)])\n",
    "\n",
    "for p, out in prompt_to_output.items():\n",
    "    attr = prompts_to_meta_data[p]['attr']\n",
    "    entity = prompts_to_meta_data[p]['entity']\n",
    "    label = '|'.join(set([x[attr] for x in entity_attributes[entity] if x[attr]]))\n",
    "    if not label:\n",
    "        continue\n",
    "    norm_label = label.lower()\n",
    "    norm_out = out.split('\"')[0].strip(' \"').replace('\\\\/', '/').lower()\n",
    "    \n",
    "    if len(norm_label) < len(norm_out):\n",
    "        correct = int(norm_out.startswith(norm_label))\n",
    "    else:\n",
    "        correct = int(norm_label.startswith(norm_out))\n",
    "    \n",
    "    if re.search('United States|United Kingdom', label):\n",
    "        norm_label = label.strip().replace('the ', '')\n",
    "        norm_out = out[len(p):].strip().replace('the ', '')\n",
    "        correct = int(norm_out.startswith(norm_label) or norm_out.startswith('England'))\n",
    "    if re.search('South Korea', label):\n",
    "        correct = int(norm_out.startswith('korea') or norm_out.startswith('south korea'))\n",
    "    if re.search('North America', label):\n",
    "        correct = norm_label in norm_out or norm_out == 'na' or norm_out.startswith('america')\n",
    "    if re.search('Mandarin', label):\n",
    "        correct = norm_out in norm_label or norm_out == 'chinese'\n",
    "    if re.search('language', p) and ',' in norm_label:\n",
    "        correct = any(lang in norm_out for lang in norm_label.split(','))\n",
    "    stats[sorted_entity.index(prompts_to_meta_data[p]['entity']), sorted_template.index(prompts_to_meta_data[p]['template'])] += int(correct)\n",
    "\n",
    "print('-----------------------------------')\n",
    "for i in np.argsort(stats.sum(axis=0))[::-1]:\n",
    "    print(sorted_template[i], int(stats[:, i].sum()), len(stats[:, i]))\n",
    "for i in np.argsort(stats.sum(axis=-1))[::-1]:\n",
    "    print(sorted_entity[i], int(stats[i].sum()), len(stats[i]))\n",
    "\n",
    "# No filtering is applied. All entities and templates are retained.\n",
    "kept_entity_index = list(range(len(sorted_entity)))  # Keeping all entities\n",
    "KEPT_ENTITY = sorted_entity\n",
    "kept_template_index = list(range(len(sorted_template)))  # Keeping all templates\n",
    "\n",
    "KEPT_ATTR_TO_PROMPT_AND_SPLIT = {}\n",
    "for attr in attribute_prompts:\n",
    "    if not attr:\n",
    "        continue\n",
    "    attr_indices = [sorted_template.index(t) for t in attribute_prompts[attr]]\n",
    "    KEPT_ATTR_TO_PROMPT_AND_SPLIT[attr] = {sorted_template[i]: prompt_splits[sorted_template[i]] for i in attr_indices}\n",
    "\n",
    "print('Kept %d entity, %d prompt template' % (len(kept_entity_index), len(kept_template_index)))\n",
    "print('Average accuracy: %.2f%%' % (100 * (stats[:, kept_template_index][kept_entity_index, :]).sum()/ (len(kept_entity_index) * len(kept_template_index))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kept templates\n",
    "\n",
    "attribute_prompts\n",
    "for i in kept_template_index:\n",
    "  print(f'{prompt_splits[sorted_template[i]]}\\t{sorted_template[i]}\\t{stats[:, i][kept_entity_index].mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Country'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'train': 22, 'test': 13, 'val': 11})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Continent'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'train': 22, 'test': 11, 'val': 10})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Language'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'train': 20, 'val': 13, 'test': 11})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sum(map(len, KEPT_ATTR_TO_PROMPT_AND_SPLIT.values())))\n",
    "for attr, prompt_to_split in KEPT_ATTR_TO_PROMPT_AND_SPLIT.items():\n",
    "  display(attr, collections.Counter(prompt_to_split.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ENTITY_TYPE = 'city'\n",
    "\n",
    "# Filtered\n",
    "KEPT_ENTITY_SPLITS = {e: entity_splits[e] for e in KEPT_ENTITY}\n",
    "KEPT_PROMPT_SPLITS = {k: (a, v) for a, d in KEPT_ATTR_TO_PROMPT_AND_SPLIT.items() for k, v in d.items() if k.count('%') == 1}\n",
    "# Wikipedia prompts\n",
    "for prompt in attribute_prompts['']:\n",
    "  KEPT_PROMPT_SPLITS[prompt] = ('Other', prompt_splits[prompt])\n",
    "KEPT_ATTR_TO_PROMPT_AND_SPLIT = {k: {p: v for p, v in d.items() if p.count('%') == 1} for k, d in KEPT_ATTR_TO_PROMPT_AND_SPLIT.items()}\n",
    "print(f'Total #entities={len(entity_attributes)} #attributes={len(KEPT_ATTR_TO_PROMPT_AND_SPLIT)} '\n",
    "      f'#prompts={sum(map(len, attribute_prompts.values()))} #wiki_prompts={len(attribute_prompts[\"\"])}')\n",
    "print(f'Kept #entities={len(KEPT_ENTITY_SPLITS)} #prompts={len(KEPT_PROMPT_SPLITS)}')\n",
    "for split in ('train', 'val', 'test'):\n",
    "  print(split, f'Kept #entities={len([k for k, v in KEPT_ENTITY_SPLITS.items() if v == split])}',\n",
    "               f'#prompts={len([k for k, v in KEPT_PROMPT_SPLITS.items() if v[1] == split])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3615235/2197762071.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wiki_prompt_to_output = torch.load(os.path.join(DATA_DIR, 'ravel_wiki_city_prompt_to_output.pt'))\n"
     ]
    }
   ],
   "source": [
    "from src.utils.generation_utils import generate_batched\n",
    "\n",
    "wiki_prompts = [(t['Template'] % e)\n",
    "                for s in ravel_city_prompts\n",
    "                for t in ravel_city_prompts[s]\n",
    "                for e in ([t['Entity']] if t['Entity']\n",
    "                           else [a for a in KEPT_ENTITY_SPLITS if KEPT_ENTITY_SPLITS[a] == 'train' or s == 'train'])\n",
    "                 ]\n",
    "print(len(wiki_prompts))\n",
    "\n",
    "# wiki_prompt_and_output = generate_batched(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     wiki_prompts,\n",
    "#     max_new_tokens=8,\n",
    "#     batch_size=64)\n",
    "# wiki_prompt_to_output = {k: v[len(k):] for k, v in wiki_prompt_and_output}\n",
    "\n",
    "wiki_prompt_to_output = torch.load(os.path.join(DATA_DIR, 'ravel_wiki_city_prompt_to_output.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(wiki_prompt_to_output, os.path.join(DATA_DIR, 'ravel_wiki_city_prompt_to_output.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472408"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_PROMPT_TO_OUTPUT = {**prompt_to_output, **wiki_prompt_to_output}\n",
    "\n",
    "len(ALL_PROMPT_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_PROMPT_SPLITS = {\n",
    "    t['Template']: {'split': s, 'entity': t['Entity']}\n",
    "    for s in ravel_city_prompts\n",
    "    for t in ravel_city_prompts[s]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "from src.utils.generate_ravel_instance import RAVELMetadata\n",
    "\n",
    "\n",
    "def extract_label(text):\n",
    "  \"\"\"Extracts the first word or phrase from the text.\n",
    "\n",
    "  The rules are hard-coded based on the model output values.\n",
    "  You might want to update the rules when using different models or prompts.\n",
    "  \"\"\"\n",
    "  tokens = re.split(r'([\"]|[.,;]\\s|\\n| \\(|\\sand)', text + ' ')\n",
    "  x = tokens[0]\n",
    "  digit_match = re.search(r'\\.\\d\\d', x)\n",
    "  if digit_match:\n",
    "      x = x[:digit_match.span(0)[1]]\n",
    "  gender_match = re.match(r'\\s?(his|her|himself|herself|she|he)[^\\w]', x)\n",
    "  if gender_match:\n",
    "      x = x[:gender_match.span(1)[1]]\n",
    "  if not x.strip():\n",
    "      x = ' '.join(text.split(' ')[:2]).rstrip('.,\"\\n')\n",
    "  assert x.strip()\n",
    "  return x\n",
    "\n",
    "\n",
    "def get_first_token(x):\n",
    "  return re.split(r'[^\\w\\+\\-]', x.strip(), re.UNICODE)[0]\n",
    "\n",
    "\n",
    "def filter_inv_example(base_output, inv_output):\n",
    "  different_outputs = (get_first_token(base_output) !=\n",
    "                       get_first_token(inv_output))\n",
    "  valid_outputs = (\n",
    "      re.fullmatch(r'\\s?[a-z0-9.:\\-+]+', extract_label(base_output), re.IGNORECASE) and\n",
    "      re.fullmatch(r'\\s?[a-z0-9.:\\-+]+', extract_label(inv_output), re.IGNORECASE))\n",
    "  return len(inv_output) > 0 and valid_outputs and different_outputs\n",
    "\n",
    "\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "ravel_metadata = RAVELMetadata(\n",
    "    'gemma-2-2b',\n",
    "    KEPT_ENTITY_SPLITS,\n",
    "    KEPT_ATTR_TO_PROMPT_AND_SPLIT,\n",
    "    KEPT_PROMPT_SPLITS,\n",
    "    WIKI_PROMPT_SPLITS,\n",
    "    ALL_PROMPT_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate context test/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate the Conetxt TEST/VAL Split\n",
    "\n",
    "# Context Split: All entities are in TRAIN, but all prompts are in test/dev\n",
    "\n",
    "import random\n",
    "\n",
    "from src.utils.generate_ravel_instance import gen_context_test_split\n",
    "\n",
    "TEST_TYPE = 'context'\n",
    "\n",
    "# Take the first N examples only\n",
    "first_n = 256\n",
    "\n",
    "eval_split_to_raw_example = gen_context_test_split(\n",
    "    ravel_metadata,\n",
    "    extract_label_fn=extract_label,\n",
    "    filter_example_fn=filter_inv_example,\n",
    "    first_n=first_n)\n",
    "eval_split_to_dataset = {\n",
    "    split: Dataset.from_list(eval_split_to_raw_example[split][:first_n], features=FEATURE_TYPES)\n",
    "    for split in eval_split_to_raw_example}\n",
    "\n",
    "# Compute stats.\n",
    "for split in eval_split_to_raw_example:\n",
    "  print('\\nSplit %s:\\nTotal %d examples, kept first %d examples, %d unique input values,  %d unique entities, %d unique output values, %d unique output tokens' % (\n",
    "      repr(split), len(eval_split_to_raw_example[split]), len(eval_split_to_dataset[split]),\n",
    "      len(set([exp[x] for exp in eval_split_to_raw_example[split][:first_n] for x in ['input', 'source_input']])),\n",
    "      len(set([exp[x] for exp in eval_split_to_raw_example[split][:first_n] for x in ['entity', 'source_entity']])),\n",
    "      len(set([exp['inv_label'] for exp in eval_split_to_raw_example[split][:first_n]])),\n",
    "      len(set([tokenizer(exp['inv_label']).input_ids[1] for exp in eval_split_to_raw_example[split][:first_n]]))))\n",
    "  #for i, example in enumerate(eval_split_to_raw_example[split]):\n",
    "  #  print(example)\n",
    "  #  #print(tokenizer(example['input']).input_ids)\n",
    "  #  break\n",
    "  #for k in ('input', 'source_input'):\n",
    "  #  input_ids = tokenizer(example[k])['input_ids']\n",
    "  #  #print(k)\n",
    "  #  #print(input_ids)\n",
    "  #  print(list(zip([(32 - len(input_ids)) + i for i in range(len(input_ids))], tokenizer.batch_decode(input_ids))))\n",
    "for split in ('test', 'val'):\n",
    "  print(f'Split {split}: Total #subsplit={len([k for k in eval_split_to_raw_example if k.endswith(split)])} #Examples={sum(map(len, [v for k, v in eval_split_to_raw_example.items() if k.endswith(split)]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge subsplits\n",
    "eval_split_to_raw_example_merged = collections.defaultdict(list)\n",
    "for split in eval_split_to_raw_example:\n",
    "  eval_split_to_raw_example_merged[re.sub(r'-causal|-output|-other', '', split)].extend(eval_split_to_raw_example[split])\n",
    "eval_split_to_raw_example = dict(eval_split_to_raw_example_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = os.path.join(DATA_DIR, f'{ravel_metadata.instance}/{ravel_metadata.instance}_{ENTITY_TYPE}_{TEST_TYPE}_test.json')\n",
    "print(output_json_path)\n",
    "json.dump(eval_split_to_raw_example, open(output_json_path, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate entity test/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate the Entity TEST/VAL Split\n",
    "\n",
    "from src.utils.generate_ravel_instance import gen_entity_test_split\n",
    "\n",
    "TEST_TYPE = 'entity'\n",
    "\n",
    "# Take the first N examples only\n",
    "first_n = 256\n",
    "\n",
    "eval_split_to_raw_example = gen_entity_test_split(\n",
    "    ravel_metadata,\n",
    "    extract_label_fn=extract_label, filter_example_fn=filter_inv_example,\n",
    "    first_n=first_n)\n",
    "\n",
    "eval_split_to_dataset = {\n",
    "    split: Dataset.from_list(eval_split_to_raw_example[split][:first_n], features=FEATURE_TYPES)\n",
    "    for split in eval_split_to_raw_example}\n",
    "\n",
    "# Stats\n",
    "for split in eval_split_to_raw_example:\n",
    "  print('Split %s: Total %d examples, kept first %d examples, %d unique input values,  %d unique entities, %d unique output values, %d unique output tokens' % (\n",
    "      repr(split), len(eval_split_to_raw_example[split]), len(eval_split_to_dataset[split]),\n",
    "      len(set([exp[x] for exp in eval_split_to_raw_example[split][:first_n] for x in ['input', 'source_input']])),\n",
    "      len(set([exp[x] for exp in eval_split_to_raw_example[split][:first_n] for x in ['entity', 'source_entity']])),\n",
    "      len(set([exp['inv_label'] for exp in eval_split_to_raw_example[split][:first_n]])),\n",
    "      len(set([tokenizer(exp['inv_label']).input_ids[1] for exp in eval_split_to_raw_example[split][:first_n]]))))\n",
    "  for i, example in enumerate(eval_split_to_raw_example[split]):\n",
    "    print(example)\n",
    "    #print(tokenizer(example['input']).input_ids)\n",
    "    break\n",
    "  for k in ('input', 'source_input'):\n",
    "    input_ids = tokenizer(example[k])['input_ids']\n",
    "    #print(k)\n",
    "    #print(input_ids)\n",
    "    print(list(zip([(32 - len(input_ids)) + i for i in range(len(input_ids))], tokenizer.batch_decode(input_ids))))\n",
    "for split in ('test', 'val'):\n",
    "  print(f'Split {split}: Total #subsplit={len([k for k in eval_split_to_raw_example if k.endswith(split)])} #Examples={sum(map(len, [v for k, v in eval_split_to_raw_example.items() if k.endswith(split)]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge subsplits\n",
    "eval_split_to_raw_example_merged = collections.defaultdict(list)\n",
    "for split in eval_split_to_raw_example:\n",
    "  eval_split_to_raw_example_merged[re.sub(r'-causal|-output|-other', '', split)].extend(eval_split_to_raw_example[split])\n",
    "eval_split_to_raw_example = dict(eval_split_to_raw_example_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = os.path.join(DATA_DIR, f'{ravel_metadata.instance}/{ravel_metadata.instance}_{ENTITY_TYPE}_{TEST_TYPE}_test.json')\n",
    "print(output_json_path)\n",
    "json.dump(eval_split_to_raw_example, open(output_json_path, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate train split (for models that use counterfactuals)\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_train_split(metadata, extract_label_fn, filter_example_fn, first_n=256):\n",
    "  split_to_raw_example = {}\n",
    "  # Group by attributes.\n",
    "  target_split = 'train'\n",
    "  for attr, prompt_to_split in metadata.attr_to_prompt.items():\n",
    "      base_prompt_candiates = [p for p, s in prompt_to_split.items() if s == target_split]\n",
    "      base_task_inputs = [\n",
    "          ((prompt, entity), metadata.prompt_to_output[prompt % entity])\n",
    "          for entity in metadata.get_entities(target_split)\n",
    "          for prompt in random.sample(\n",
    "              base_prompt_candiates, k=min(2, len(base_prompt_candiates)))]\n",
    "      print(len(base_task_inputs))\n",
    "      source_task_inputs = [\n",
    "          ((source_prompt, entity), metadata.prompt_to_output[source_prompt % entity])\n",
    "          for source_prompt, (source_attr, source_split) in KEPT_PROMPT_SPLITS.items()\n",
    "          if source_split == target_split and source_attr != 'Other'\n",
    "          for entity in metadata.sample_entities(target_split, k=1)\n",
    "      ]\n",
    "      wiki_source_task_inputs = [\n",
    "          ((source_prompt, entity), metadata.prompt_to_output[source_prompt % entity])\n",
    "          for source_prompt, split_and_arg in metadata.entity_prompt_to_split.items()\n",
    "          if split_and_arg['split'] == target_split\n",
    "          for entity in ([split_and_arg['entity']] if split_and_arg['entity']\n",
    "                         else metadata.sample_entities(target_split, k=1))\n",
    "      ]\n",
    "      source_task_inputs = source_task_inputs + wiki_source_task_inputs\n",
    "      if len(base_task_inputs) < 5 or len(source_task_inputs) < 5:\n",
    "        continue\n",
    "      print(attr, target_split, len(base_task_inputs), len(source_task_inputs), len(wiki_source_task_inputs))\n",
    "      split_to_raw_example[f'{attr}-{target_split}'] = []\n",
    "      for (p, a), v in base_task_inputs:\n",
    "        source_input_candiates = [x for x in source_task_inputs if\n",
    "                                  x[0][1] in ravel_metadata.entity_to_split and\n",
    "                                  filter_example_fn(v, metadata.prompt_to_output[p % x[0][1]])]\n",
    "        #print(len(source_input_candiates), v)\n",
    "        split_to_raw_example[f'{attr}-{target_split}'].extend([{\n",
    "          'input': p % a, 'label': extract_label_fn(v),\n",
    "          'source_input': s_p % s_a, 'source_label': extract_label_fn(source_v),\n",
    "          'inv_label': extract_label_fn(metadata.prompt_to_output[p % s_a]),\n",
    "          'split': p, 'source_split': s_p,\n",
    "          'entity': a, 'source_entity': s_a}\n",
    "        for (s_p, s_a), source_v in random.sample(source_input_candiates, k=min(len(source_input_candiates), round(first_n / len(base_task_inputs))))\n",
    "        if filter_example_fn(v, metadata.prompt_to_output[p % s_a]) and re.search('\\w+', source_v)\n",
    "      ])\n",
    "  split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "  return split_to_raw_example\n",
    "\n",
    "\n",
    "# Take the first N examples only\n",
    "first_n = 40960\n",
    "\n",
    "split_to_raw_example = gen_train_split(\n",
    "    ravel_metadata,\n",
    "    extract_label_fn=extract_label,\n",
    "    filter_example_fn=filter_inv_example,\n",
    "    first_n=first_n)\n",
    "\n",
    "# Stats\n",
    "for split in split_to_raw_example:\n",
    "  print('Split %s: Total %d examples, kept first %d examples, %d unique input values,  %d unique entities, %d unique output values, %d unique output tokens' % (\n",
    "      repr(split), len(split_to_raw_example[split]), len(split_to_raw_example[split]),\n",
    "      len(set([exp[x] for exp in split_to_raw_example[split][:first_n] for x in ['input', 'source_input']])),\n",
    "      len(set([exp[x] for exp in split_to_raw_example[split][:first_n] for x in ['entity', 'source_entity']])),\n",
    "      len(set([exp['inv_label'] for exp in split_to_raw_example[split][:first_n]])),\n",
    "      len(set([tokenizer(exp['inv_label']).input_ids[1] for exp in split_to_raw_example[split][:first_n]]))))\n",
    "  for i, example in enumerate(split_to_raw_example[split]):\n",
    "    print(example)\n",
    "    break\n",
    "for split in ('train',):\n",
    "  print(f'Split {split}: Total #subsplit={len([k for k in split_to_raw_example if k.endswith(split)])} #Examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith(split)]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(DATA_DIR, f'{ravel_metadata.instance}/{ravel_metadata.instance}_{ENTITY_TYPE}_train.json')\n",
    "print(json_path)\n",
    "json.dump(split_to_raw_example, open(json_path, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Postprocess labels\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "entity_type = 'city'\n",
    "instance =  'gemma-2-2b'\n",
    "\n",
    "\n",
    "json_path = os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_context_test.json')\n",
    "split_to_raw_example = json.load(open(json_path, 'r'))\n",
    "print(len(split_to_raw_example))\n",
    "\n",
    "all_labels = set()\n",
    "for split in split_to_raw_example:\n",
    "  for i in range(len(split_to_raw_example[split])):\n",
    "    # if split.split('-')[0] in ['Latitude', 'Longitude'] or  split.split('-')[0] in attribute_prompts['Latitude'] or split.split('-')[0] in attribute_to_prompts['Longitude']:\n",
    "    #   # Keep only the integer part.\n",
    "    #   split_to_raw_example[split][i]['inv_label'] = split_to_raw_example[split][i]['inv_label'].replace('°', '.').split('.')[0]\n",
    "    #   split_to_raw_example[split][i]['label'] = split_to_raw_example[split][i]['label'].replace('°', '.').split('.')[0]\n",
    "    all_labels.add(split_to_raw_example[split][i]['inv_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_path)\n",
    "json.dump(split_to_raw_example, open(json_path, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Intervention locations for all possible prompts\n",
    "\n",
    "SPLIT_TO_INV_POSITION = {}\n",
    "\n",
    "all_prompt_templates = {p for p in WIKI_PROMPT_SPLITS}\n",
    "# all_prompt_templates.update({v for vs in ALL_ATTR_TO_PROMPTS.values() for v in vs})\n",
    "print(len(all_prompt_templates))\n",
    "\n",
    "for prompt_template in all_prompt_templates:\n",
    "  if prompt_template.count('%s') != 1:\n",
    "    continue\n",
    "  print(prompt_template)\n",
    "  prompt_input = prompt_template.replace('%s', '000000', 1)\n",
    "  input_ids = tokenizer(prompt_input)['input_ids']\n",
    "  toks = tokenizer.batch_decode(input_ids)\n",
    "  for i in range(-1, -len(toks), -1):\n",
    "    # This check only works for TinyLlama/Llama2 tokenizer.\n",
    "    # If you use a different tokenizer, you need to update the code below\n",
    "    # based on how 000000 is tokenized or use a different placeholder.\n",
    "    if toks[i] == '0' and toks[i - 1] == '0' and toks[i - 2] == '0' and toks[i - 3] == '0':\n",
    "      break\n",
    "  SPLIT_TO_INV_POSITION[prompt_template] = i\n",
    "  print(i, list(zip([(32 - len(input_ids)) + i for i in range(len(input_ids))], toks)))\n",
    "\n",
    "print(min(SPLIT_TO_INV_POSITION.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SPLIT_TO_INV_POSITION,\n",
    "          open(os.path.join(DATA_DIR, instance, f'{instance}_{entity_type}_prompt_to_entity_position.json'), 'w'),\n",
    "          ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create instance for MIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "instance = 'gemma-2-2b'\n",
    "entity_type = 'city'\n",
    "INPUT_MAX_LEN = 48\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "# Load training and test datasets. \n",
    "split_to_raw_example = json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_train.json'), 'r'))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_context_test.json'), 'r')))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_entity_test.json'), 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend an extra token to avoid tokenization changes for Llama tokenizer.\n",
    "# Each sequence will start with <s> _ 0\n",
    "SOS_PAD = '0'\n",
    "NUM_SOS_TOKENS = 3\n",
    "for split in split_to_raw_example:\n",
    "  for i in range(len(split_to_raw_example[split])):\n",
    "    split_to_raw_example[split][i]['inv_label'] = SOS_PAD + split_to_raw_example[split][i]['inv_label']\n",
    "    split_to_raw_example[split][i]['label'] = SOS_PAD + split_to_raw_example[split][i]['label']\n",
    "\n",
    "\n",
    "# Load attributes (tasks) to prompt mapping.\n",
    "# ALL_ATTR_TO_PROMPTS = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "# Load prompt to intervention location mapping.\n",
    "split_to_entity_pos = json.load(open(os.path.join(DATA_DIR, instance, f'{instance}_{entity_type}_prompt_to_entity_position.json')))\n",
    "SPLIT_TO_INV_LOCATIONS = {\n",
    "    f'{task}{split}': {'max_input_length': INPUT_MAX_LEN,\n",
    "                       'inv_position': [INPUT_MAX_LEN + pos]}\n",
    "    for task, pos in split_to_entity_pos.items()\n",
    "    for split in ('-train', '-test', '-val', '')\n",
    "}\n",
    "assert(min([min(v['inv_position']) for v in SPLIT_TO_INV_LOCATIONS.values()]) > 0)\n",
    "\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def filter_inv_example(example):\n",
    "  return (example['label'] != example['inv_label'] and\n",
    "          example['source_split'] in SPLIT_TO_INV_LOCATIONS and\n",
    "          example['split'] in SPLIT_TO_INV_LOCATIONS)\n",
    "\n",
    "for split in split_to_raw_example:\n",
    "  random.shuffle(split_to_raw_example[split])\n",
    "  split_to_raw_example[split] = list(filter(filter_inv_example, split_to_raw_example[split]))\n",
    "  if len(split_to_raw_example[split]) == 0:\n",
    "    print('Empty split: \"%s\"' % split)\n",
    "# Remove empty splits.\n",
    "split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "print(f\"#Training examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-train')]))}, \"\n",
    "      f\"#Validation examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-val')]))}, \"\n",
    "      f\"#Test examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-test')]))}\")\n",
    "split_to_dataset = {split: Dataset.from_list(\n",
    "    split_to_raw_example[split], features=FEATURE_TYPES)\n",
    "                    for split in split_to_raw_example}\n",
    "\n",
    "# #Training examples=116728, #Validation examples=20516, #Test examples=22497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(split_to_raw_example, open(\"mib_ravel.json\", \"w\"), indent=4)\n",
    "# split_to_raw_example = json.load(open(\"mib_ravel.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "from collections import Counter, defaultdict\n",
    "data = split_to_raw_example[\"Country-train\"]\n",
    "input_values = [entry[\"input\"] for entry in data]\n",
    "input_counts = Counter(input_values)\n",
    "\n",
    "for text, count in input_counts.items():\n",
    "    print(f'\"{text}\" appears {count} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in split_to_raw_example.keys():\n",
    "    display(len(split_to_raw_example[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mib_ravel_train = []\n",
    "mib_ravel_test = []\n",
    "mib_ravel_val = []\n",
    "\n",
    "for key in split_to_raw_example.keys():\n",
    "    if \"-train\" in key:\n",
    "        mib_ravel_train.extend(split_to_raw_example[key])\n",
    "    if \"-test\" in key:\n",
    "        mib_ravel_test.extend(split_to_raw_example[key])\n",
    "    if \"-val\" in key:\n",
    "        mib_ravel_val.extend(split_to_raw_example[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attribute and source_attribute label to the splits\n",
    "# Train\n",
    "template_to_attribute = {\n",
    "    template: attr for template, attr in zip(ravel_city_prompts[\"train\"][\"Template\"], ravel_city_prompts[\"train\"][\"Attribute\"])\n",
    "}\n",
    "\n",
    "for entry in mib_ravel_train:\n",
    "    template = entry.get(\"split\")\n",
    "    if template in template_to_attribute:\n",
    "        entry[\"attribute\"] = template_to_attribute[template]\n",
    "\n",
    "for entry in mib_ravel_train:\n",
    "    template = entry.get(\"source_split\")\n",
    "    if template in template_to_attribute:\n",
    "        entry[\"source_attribute\"] = template_to_attribute[template] if template_to_attribute[template] != \"\" else \"wikipedia\"\n",
    "\n",
    "\n",
    "# Test\n",
    "template_to_attribute = {\n",
    "    template: attr for template, attr in zip(ravel_city_prompts[\"test\"][\"Template\"], ravel_city_prompts[\"test\"][\"Attribute\"])\n",
    "}\n",
    "\n",
    "filtered_mib_ravel_test = []\n",
    "\n",
    "for entry in mib_ravel_test:\n",
    "    template = entry.get(\"split\")\n",
    "    source_template = entry.get(\"source_split\")\n",
    "\n",
    "    if template in template_to_attribute and source_template in template_to_attribute:\n",
    "        entry[\"attribute\"] = template_to_attribute[template]\n",
    "        entry[\"source_attribute\"] = (\n",
    "            template_to_attribute[source_template] if template_to_attribute[source_template] != \"\" else \"wikipedia\"\n",
    "        )\n",
    "        filtered_mib_ravel_test.append(entry) \n",
    "\n",
    "mib_ravel_test = filtered_mib_ravel_test\n",
    "\n",
    "# for entry in mib_ravel_test:\n",
    "#     template = entry.get(\"split\")\n",
    "#     if template in template_to_attribute:\n",
    "#         entry[\"attribute\"] = template_to_attribute[template]\n",
    "\n",
    "# for entry in mib_ravel_test:\n",
    "#     template = entry.get(\"source_split\")\n",
    "#     if template in template_to_attribute:\n",
    "#         entry[\"source_attribute\"] = template_to_attribute[template] if template_to_attribute[template] != \"\" else \"wikipedia\"\n",
    "\n",
    "\n",
    "# Val\n",
    "template_to_attribute = {\n",
    "    template: attr for template, attr in zip(ravel_city_prompts[\"val\"][\"Template\"], ravel_city_prompts[\"val\"][\"Attribute\"])\n",
    "}\n",
    "filtered_mib_ravel_val = []\n",
    "\n",
    "for entry in mib_ravel_val:\n",
    "    template = entry.get(\"split\")\n",
    "    source_template = entry.get(\"source_split\")\n",
    "\n",
    "    if template in template_to_attribute and source_template in template_to_attribute:\n",
    "        entry[\"attribute\"] = template_to_attribute[template]\n",
    "        entry[\"source_attribute\"] = (\n",
    "            template_to_attribute[source_template] if template_to_attribute[source_template] != \"\" else \"wikipedia\"\n",
    "        )\n",
    "        filtered_mib_ravel_val.append(entry) \n",
    "\n",
    "mib_ravel_val = filtered_mib_ravel_val\n",
    "\n",
    "# for entry in mib_ravel_val:\n",
    "#     template = entry.get(\"split\")\n",
    "#     if template in template_to_attribute:\n",
    "#         entry[\"attribute\"] = template_to_attribute[template]\n",
    "\n",
    "# for entry in mib_ravel_val:\n",
    "#     template = entry.get(\"source_split\")\n",
    "#     if template in template_to_attribute:\n",
    "#         entry[\"source_attribute\"] = template_to_attribute[template] if template_to_attribute[template] != \"\" else \"wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat train\n",
    "grouped_train = defaultdict(list)\n",
    "for entry in mib_ravel_train:\n",
    "    grouped_train[entry[\"input\"]].append(entry)\n",
    "\n",
    "clean_ravel_train = []\n",
    "for input_value, entries in grouped_train.items():\n",
    "    # Take the first instance in the list\n",
    "    og_instance = entries[0]\n",
    "    \n",
    "    # Construct the OG dictionary\n",
    "    clean_entry = {\n",
    "        \"template\": og_instance[\"split\"],\n",
    "        \"prompt\": og_instance[\"input\"],\n",
    "        \"label\": og_instance[\"label\"],\n",
    "        \"entity\": og_instance[\"entity\"],\n",
    "        \"attribute\": og_instance[\"attribute\"],\n",
    "        \"prompt_template_counterfactual\": {},\n",
    "        \"attribute_counterfactual\": {},\n",
    "        \"wikipedia_counterfactual\": {}\n",
    "    }\n",
    "    \n",
    "    for entry in entries:\n",
    "        if entry[\"source_attribute\"] == og_instance[\"attribute\"]:\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"prompt_template_counterfactual\"] = counterfactual\n",
    "        \n",
    "        if entry[\"source_attribute\"] != og_instance[\"attribute\"] and entry[\"source_attribute\"] != \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"attribute_counterfactual\"] = counterfactual\n",
    "            \n",
    "        if entry[\"source_attribute\"] == \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"wikipedia_counterfactual\"] = counterfactual\n",
    "\n",
    "    clean_ravel_train.append(clean_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat test\n",
    "grouped_test = defaultdict(list)\n",
    "for entry in mib_ravel_test:\n",
    "    grouped_test[entry[\"input\"]].append(entry)\n",
    "\n",
    "clean_ravel_test = []\n",
    "for input_value, entries in grouped_test.items():\n",
    "    og_instance = entries[0]\n",
    "    \n",
    "    # Construct the OG dictionary\n",
    "    try: \n",
    "        clean_entry = {\n",
    "            \"template\": og_instance[\"split\"],\n",
    "            \"prompt\": og_instance[\"input\"],\n",
    "            \"label\": og_instance[\"label\"],\n",
    "            \"entity\": og_instance[\"entity\"],\n",
    "            \"attribute\": og_instance[\"attribute\"],\n",
    "            \"prompt_template_counterfactual\": {},\n",
    "            \"attribute_counterfactual\": {},\n",
    "            \"wikipedia_counterfactual\": {}\n",
    "        }\n",
    "    except KeyError:\n",
    "        display(og_instance)\n",
    "    \n",
    "    for entry in entries:\n",
    "        if entry[\"source_attribute\"] == og_instance[\"attribute\"]:\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"prompt_template_counterfactual\"] = counterfactual\n",
    "        \n",
    "        if entry[\"source_attribute\"] != og_instance[\"attribute\"] and entry[\"source_attribute\"] != \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"attribute_counterfactual\"] = counterfactual\n",
    "            \n",
    "        if entry[\"source_attribute\"] == \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"wikipedia_counterfactual\"] = counterfactual\n",
    "\n",
    "    clean_ravel_test.append(clean_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat val\n",
    "grouped_val = defaultdict(list)\n",
    "for entry in mib_ravel_val:\n",
    "    grouped_val[entry[\"input\"]].append(entry)\n",
    "\n",
    "clean_ravel_val = []\n",
    "for input_value, entries in grouped_val.items():\n",
    "    og_instance = entries[0]\n",
    "    \n",
    "    # Construct the OG dictionary\n",
    "    try: \n",
    "        clean_entry = {\n",
    "            \"template\": og_instance[\"split\"],\n",
    "            \"prompt\": og_instance[\"input\"],\n",
    "            \"label\": og_instance[\"label\"],\n",
    "            \"entity\": og_instance[\"entity\"],\n",
    "            \"attribute\": og_instance[\"attribute\"],\n",
    "            \"prompt_template_counterfactual\": {},\n",
    "            \"attribute_counterfactual\": {},\n",
    "            \"wikipedia_counterfactual\": {}\n",
    "        }\n",
    "    except KeyError:\n",
    "        display(og_instance)\n",
    "    \n",
    "    for entry in entries:\n",
    "        if entry[\"source_attribute\"] == og_instance[\"attribute\"]:\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"prompt_template_counterfactual\"] = counterfactual\n",
    "        \n",
    "        if entry[\"source_attribute\"] != og_instance[\"attribute\"] and entry[\"source_attribute\"] != \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"attribute_counterfactual\"] = counterfactual\n",
    "            \n",
    "        if entry[\"source_attribute\"] == \"wikipedia\":\n",
    "            counterfactual = {\n",
    "                \"label\": entry[\"source_label\"],\n",
    "                \"prompt\": entry[\"source_input\"]\n",
    "            }\n",
    "            clean_entry[\"wikipedia_counterfactual\"] = counterfactual\n",
    "\n",
    "    clean_ravel_val.append(clean_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test and val sets are much larger than train set\n",
    "\n",
    "random.shuffle(clean_ravel_val)\n",
    "mib_ravel_val_split = clean_ravel_val[:len(clean_ravel_val)//2]\n",
    "mib_ravel_public_test = clean_ravel_val[len(clean_ravel_val)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575ffcfaff5b4f088cba39514f61ee9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3d30805dc7477f93549d94ef942de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd3323238144bc2866ff1d854ee1ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d5765a8df14e7886d0adf896bf68d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae20c05ee3845fb81e40b2d30460816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7773fb63f8e4849970bf0ee54ae33c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5feaeaa0e648abbfd0b6edff178ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/828 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yiksiu/mib_ravel/commit/1852e673b33845ab818874e9cea79834ff38932e', commit_message='Upload dataset', commit_description='', oid='1852e673b33845ab818874e9cea79834ff38932e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/yiksiu/mib_ravel', endpoint='https://huggingface.co', repo_type='dataset', repo_id='yiksiu/mib_ravel'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ravel_train = Dataset.from_list(clean_ravel_train)\n",
    "mib_ravel_val_split = Dataset.from_list(mib_ravel_val_split)\n",
    "mib_ravel_public_test = Dataset.from_list(mib_ravel_public_test)\n",
    "\n",
    "mib_ravel = DatasetDict({\"train\": clean_ravel_train,                        \n",
    "                         \"val\": mib_ravel_val_split,\n",
    "                         \"test\": mib_ravel_public_test})\n",
    "mib_ravel.push_to_hub(\"yiksiu/mib_ravel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
